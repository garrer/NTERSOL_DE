{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garrer/NTERSOL_DE/blob/main/Copy_of_SparkTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Code required to get Spark environment"
      ],
      "metadata": {
        "id": "Bl9jWbO3BQfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "unxEZkZSUDME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ef9fe17-c8cd-4337-c4df-22b7d6ffb6f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 41 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 58.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845513 sha256=4e07733c63f0e26cdd75e932a411a8a69d01ce379bbd44ea87dee94ca87dd16f\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "nsFVKmllUGdZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "LSPhc-2ZWyD5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "87daf2ee-2822-4989-d683-f53198a9c26d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f449ac651d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://9f2929f8bdac:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = spark.read.csv('sample_data/data-engineer-interview-data.csv', header=True)"
      ],
      "metadata": {
        "id": "as5tdLNGqkga"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df.createOrReplaceTempView(\"TitleData\")"
      ],
      "metadata": {
        "id": "qJCK3348qm7D"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from TitleData limit 10\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0YU2nROqvrl",
        "outputId": "9133421b-790e-4759-c1b5-5c577e00b186"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[DocumentNumber: string, DocumentDate: string, DocumentType: string, RefersToDocumentNumber: string, RefersToDocumentYear: string, Remarks: string]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwSvuQycrFU_",
        "outputId": "1888aefc-4497-4a09-dfa7-436bd267c9e7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------+------------+----------------------+--------------------+------------+\n",
            "|DocumentNumber|DocumentDate|DocumentType|RefersToDocumentNumber|RefersToDocumentYear|     Remarks|\n",
            "+--------------+------------+------------+----------------------+--------------------+------------+\n",
            "|            27|   8/16/2021|           C|                  null|                null|        null|\n",
            "|             1|   2/15/2021|           T|                     8|                2020|2020 0008-00|\n",
            "|            67|   10/9/2020|           A|                  null|                null|        null|\n",
            "|           157|   10/9/2020|           A|                  null|                null|        null|\n",
            "|           189|   10/9/2020|           J|                  null|                null|        null|\n",
            "|           250|   10/9/2020|           R|                  2016|                  98|        null|\n",
            "|            87|    8/7/2020|           R|                  1992|                  97|        null|\n",
            "|             8|    7/3/2020|           B|                  null|                null|        null|\n",
            "|             2|    1/5/2020|           C|                  null|                null|        null|\n",
            "|            34|   6/21/2019|           A|                  null|                null|        null|\n",
            "+--------------+------------+------------+----------------------+--------------------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql (\"\"\"\n",
        "select DocumentNumber,DocumentDate, Documenttype, RefersToDocumentNumber,RefersToDocumentYear\n",
        ", concat (DocumentNumber,split(DocumentDate,'/')[2]) as document_unique_id\n",
        ", row_number() OVER (PARTITION BY concat (DocumentNumber,split(DocumentDate,'/')[2]) ORDER BY DocumentNumber) as rn\n",
        ", CASE WHEN length (RefersToDocumentYear) < 4 THEN concat (RefersToDocumentYear,RefersToDocumentNumber) ELSE concat (RefersToDocumentNumber,RefersToDocumentYear) END as document_ref_unique_id\n",
        "from TitleData\n",
        "\"\"\").createOrReplaceTempView(\"derived\")"
      ],
      "metadata": {
        "id": "Y8uPy1YyGZHv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spark.sql(\"\"\"\n",
        "               \n",
        "               \n",
        "               \n",
        "#                 select *, CAST ((DocumentNumber+split(DocumentDate,'/')[2]) AS INT) as document_unique_Id,  \n",
        "#                     CAST ( COALESCE(RefersToDocumentNumber, 0)+ COALESCE (RefersToDocumentYear, 0) AS INT) as document_ref_unique_Id                   \n",
        "#                     from TitleData\n",
        "#                     \"\"\").createOrReplaceTempView(\"derived\")"
      ],
      "metadata": {
        "id": "d0CkVg78rLM_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"select * from derived limit 10\"\"\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEpLVqierjjt",
        "outputId": "1e327eee-10aa-4a3f-8eb0-f1998780dd38"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------+------------+----------------------+--------------------+------------------+---+----------------------+\n",
            "|DocumentNumber|DocumentDate|Documenttype|RefersToDocumentNumber|RefersToDocumentYear|document_unique_id| rn|document_ref_unique_id|\n",
            "+--------------+------------+------------+----------------------+--------------------+------------------+---+----------------------+\n",
            "|           100|   8/21/2002|           J|                  null|                null|           1002002|  1|                  null|\n",
            "|           100|  12/24/2010|           B|                  null|                null|           1002010|  1|                  null|\n",
            "|           100|    2/5/2010|           C|                  null|                null|           1002010|  2|                  null|\n",
            "|           101|  11/29/2002|           A|                  null|                null|           1012002|  1|                  null|\n",
            "|            10|   7/18/1988|           B|                  null|                null|            101988|  1|                  null|\n",
            "|         10300|   3/30/1978|           J|                  null|                null|         103001978|  1|                  null|\n",
            "|           103|   7/23/2004|           J|                  null|                null|           1032004|  1|                  null|\n",
            "|           105|    8/4/2000|           J|                  null|                null|           1052000|  1|                  null|\n",
            "|           106|  12/19/2003|           C|                  null|                null|           1062003|  1|                  null|\n",
            "|           108|  12/24/2010|           T|                  2006|                  52|           1082010|  1|                522006|\n",
            "+--------------+------------+------------+----------------------+--------------------+------------------+---+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql (\"\"\"select t1.*, t2.DocumentType as ReferenceType           \n",
        "                 from derived t1 LEFT JOIN derived t2 on t1.document_ref_unique_Id = t2.document_unique_Id\"\"\"            \n",
        "               ).createOrReplaceTempView(\"derived_final\")"
      ],
      "metadata": {
        "id": "mr4c78Gisoit"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"select * from derived_final limit 10\"\"\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0VmKFEfs8Gp",
        "outputId": "59af2db0-3ce9-4576-d6a2-090bb969e154"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------+------------+----------------------+--------------------+------------------+---+----------------------+-------------+\n",
            "|DocumentNumber|DocumentDate|Documenttype|RefersToDocumentNumber|RefersToDocumentYear|document_unique_id| rn|document_ref_unique_id|ReferenceType|\n",
            "+--------------+------------+------------+----------------------+--------------------+------------------+---+----------------------+-------------+\n",
            "|           100|   8/21/2002|           J|                  null|                null|           1002002|  1|                  null|         null|\n",
            "|           100|  12/24/2010|           B|                  null|                null|           1002010|  1|                  null|         null|\n",
            "|           100|    2/5/2010|           C|                  null|                null|           1002010|  2|                  null|         null|\n",
            "|           101|  11/29/2002|           A|                  null|                null|           1012002|  1|                  null|         null|\n",
            "|            10|   7/18/1988|           B|                  null|                null|            101988|  1|                  null|         null|\n",
            "|         10300|   3/30/1978|           J|                  null|                null|         103001978|  1|                  null|         null|\n",
            "|           103|   7/23/2004|           J|                  null|                null|           1032004|  1|                  null|         null|\n",
            "|           105|    8/4/2000|           J|                  null|                null|           1052000|  1|                  null|         null|\n",
            "|           106|  12/19/2003|           C|                  null|                null|           1062003|  1|                  null|         null|\n",
            "|           108|  12/24/2010|           T|                  2006|                  52|           1082010|  1|                522006|            J|\n",
            "+--------------+------------+------------+----------------------+--------------------+------------------+---+----------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%markdown\n",
        "\n",
        "* Documents of type J (a \"judgment\" document) no longer apply if a document of type T (a \"termination\" document) refers to it\n",
        "* Documents of any other type no longer apply if a document of type R (a \"release\" document) refers to it\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "Qsq803CTtHc3",
        "outputId": "c4854ac7-1348-49d0-fe67-315eb8c254e1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n* Documents of type J (a \"judgment\" document) no longer apply if a document of type T (a \"termination\" document) refers to it\n* Documents of any other type no longer apply if a document of type R (a \"release\" document) refers to it\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql (\"\"\"\n",
        "            \n",
        "            SELECT * FROM derived_final t1 WHERE t1.document_unique_Id NOT IN (\n",
        "            SELECT  document_unique_Id  FROM derived_final t1 where (lower (t1.DocumentType) == 't' and\n",
        "            lower (t1.ReferenceType) == 'j') OR (lower (t1.ReferenceType) == 'r')) \n",
        "                \n",
        "            \"\"\").createOrReplaceTempView(\"TitleDataFinal\")"
      ],
      "metadata": {
        "id": "HiSwC_cTuL5M"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1ZBFYtuRtsHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test case: Check if TitleDataFinal has any document type J referencing to document type T\n",
        "spark.sql (\"\"\" select * from TitleDataFinal where upper(DocumentType)=='T' and upper(ReferenceType)=='J' \"\"\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ02mQFlusX-",
        "outputId": "2dd18a72-2bbd-4878-9a02-06f239d2236c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------+------------+----------------------+--------------------+------------------+---+----------------------+-------------+\n",
            "|DocumentNumber|DocumentDate|Documenttype|RefersToDocumentNumber|RefersToDocumentYear|document_unique_id| rn|document_ref_unique_id|ReferenceType|\n",
            "+--------------+------------+------------+----------------------+--------------------+------------------+---+----------------------+-------------+\n",
            "+--------------+------------+------------+----------------------+--------------------+------------------+---+----------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test case: Check if TitleDataFinal has any record referencing document type R\n",
        "spark.sql (\"\"\" select * from TitleDataFinal where upper(ReferenceType)=='R' \"\"\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyyHEUrHwC39",
        "outputId": "d786b82a-73cc-4eb1-9fac-cd85c667a0c7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------+------------+----------------------+--------------------+------------------+---+----------------------+-------------+\n",
            "|DocumentNumber|DocumentDate|Documenttype|RefersToDocumentNumber|RefersToDocumentYear|document_unique_id| rn|document_ref_unique_id|ReferenceType|\n",
            "+--------------+------------+------------+----------------------+--------------------+------------------+---+----------------------+-------------+\n",
            "+--------------+------------+------------+----------------------+--------------------+------------------+---+----------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql (\"\"\" select count(*) from TitleDataFinal \"\"\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCqmrsxRwRql",
        "outputId": "9472d3de-6bf5-46b9-d801-00b6caaa84aa"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|     142|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}